{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "from torchvision import transforms\n",
    "\n",
    "sys.path.append('../src')\n",
    "\n",
    "\n",
    "from utils.data_loader import CloudDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caminhos para as imagens e máscaras processadas\n",
    "processed_images_dir = '../data/processed/38-Cloud/images/train/'\n",
    "processed_masks_dir = '../data/processed/38-Cloud/masks/train/'\n",
    "\n",
    "# Caminhos para as divisões\n",
    "splits_dir = '../data/processed/38-Cloud/splits/'\n",
    "\n",
    "# Carregando as divisões do dataset\n",
    "with open(os.path.join(splits_dir, 'train_files.json'), 'r') as f:\n",
    "    train_files = json.load(f)\n",
    "with open(os.path.join(splits_dir, 'val_files.json'), 'r') as f:\n",
    "    val_files = json.load(f)\n",
    "with open(os.path.join(splits_dir, 'test_files.json'), 'r') as f:\n",
    "    test_files = json.load(f)\n",
    "\n",
    "print(f\"Número de imagens de treinamento: {len(train_files)}\")\n",
    "print(f\"Número de imagens de validação: {len(val_files)}\")\n",
    "print(f\"Número de imagens de teste: {len(test_files)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformações e Normalização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Médias e desvios padrão calculados anteriormente\n",
    "mean = [0.485, 0.456, 0.406]  \n",
    "std = [0.229, 0.224, 0.225]  \n",
    "\n",
    "# Transformações para as imagens\n",
    "image_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criando o Dataset de Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diretórios de imagens e máscaras de treinamento (originais e aumentadas)\n",
    "train_images_dirs = [processed_images_dir, augmented_images_dir]\n",
    "train_masks_dirs = [processed_masks_dir, augmented_masks_dir]\n",
    "\n",
    "# Combinar os arquivos de imagens e máscaras\n",
    "train_images_list = [f for f in train_files_augmented]\n",
    "train_masks_list = [f for f in train_files_augmented]\n",
    "\n",
    "# Criar instâncias do dataset para treinamento\n",
    "train_dataset = CloudDataset(\n",
    "    images_dir=processed_images_dir,\n",
    "    masks_dir=processed_masks_dir,\n",
    "    file_list=train_images_list,\n",
    "    transform=image_transforms\n",
    ")\n",
    "\n",
    "# Atualizar os diretórios para incluir as imagens aumentadas\n",
    "train_dataset.images_dir = augmented_images_dir\n",
    "train_dataset.masks_dir = augmented_masks_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criando o DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparando os Datasets de Validação e Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset de validação\n",
    "val_dataset = CloudDataset(\n",
    "    images_dir=processed_images_dir,\n",
    "    masks_dir=processed_masks_dir,\n",
    "    file_list=val_files,\n",
    "    transform=image_transforms\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "# Dataset de teste\n",
    "test_dataset = CloudDataset(\n",
    "    images_dir=processed_images_dir,\n",
    "    masks_dir=processed_masks_dir,\n",
    "    file_list=test_files,\n",
    "    transform=image_transforms\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
